{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8fc8551f",
      "metadata": {
        "id": "8fc8551f"
      },
      "source": [
        "# ðŸ™ï¸ Apartment Price Prediction â€” Interactive End-to-End ML Project  \n",
        "**Author:** Sohaib Bantan  \n",
        "**Last Updated:** 2025-10-29  \n",
        "\n",
        "This notebook presents a complete, **Machine Learning project** that predicts apartment prices using real-world tabular data.  \n",
        "It combines clean preprocessing, model experimentation, interactive exploration, and user-driven predictions â€” all in one notebook.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸš€ Project Overview\n",
        "\n",
        "The workflow includes:\n",
        "- **Data Loading & Preparation:** Load the dataset, identify features and target column, handle missing values, and split into training/testing sets.  \n",
        "- **Preprocessing:** Automatic imputation, encoding of categorical variables, and feature scaling via `ColumnTransformer`.  \n",
        "- **Model Selection:** The user interactively chooses between three models:\n",
        "  1. **Linear Regression** â€“ interpretable baseline  \n",
        "  2. **HistGradientBoostingRegressor** â€“ high-performance tree-based model  \n",
        "  3. **Neural Network (Keras)** â€“ simple feedforward deep learning model  \n",
        "- **Training & Evaluation:** Model-specific training with performance metrics:\n",
        "  - RMSE, MAE, RÂ², and custom accuracy (within Â±10%)  \n",
        "- **Visualizations:**  \n",
        "  - Predicted vs Actual plot  \n",
        "  - Residuals plot  \n",
        "  - Error distribution  \n",
        "  - Neural network training curve (when applicable)  \n",
        "- **Performance Insights:** Simple confusion-style matrix to assess over/underestimation patterns.  \n",
        "- **Interactive Prediction:** The user can manually input apartment details to get real-time price predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  How to Use\n",
        "1. Run all cells **in order** from top to bottom.  \n",
        "2. When prompted, **select a model** to train interactively.  \n",
        "3. Review metrics and visualization outputs.  \n",
        "4. Use the **manual prediction cell** at the end to input apartment details and get a predicted price instantly.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§° Requirements\n",
        "Make sure the following libraries are installed before running this notebook:\n",
        "\n",
        "```bash\n",
        "pip install -U pandas numpy matplotlib scikit-learn tensorflow ipywidgets seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I-4wa1FJb_aU",
      "metadata": {
        "id": "I-4wa1FJb_aU"
      },
      "outputs": [],
      "source": [
        "pip install -U pandas numpy scikit-learn matplotlib ipywidgets xgboost scikeras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b477dc10",
      "metadata": {
        "id": "b477dc10"
      },
      "outputs": [],
      "source": [
        "# Imports & global config\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing & model utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "\n",
        "# Deep Learning\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams['figure.figsize'] = (7, 4)\n",
        "plt.rcParams['axes.grid'] = True\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "except Exception as e:\n",
        "    widgets = None\n",
        "\n",
        "# Data source â€” update this to your local dataset if available\n",
        "DATA_CSV_PATH = os.environ.get('APARTMENTS_DATA', '')  # e.g., '/path/to/apartments_for_rent.csv'\n",
        "FALLBACK_URL = 'https://raw.githubusercontent.com/be-prado/csci4521/refs/heads/main/apartments_for_rent.csv'\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5f28f8",
      "metadata": {
        "id": "0c5f28f8"
      },
      "source": [
        "## 1. Load Dataset\n",
        "\n",
        "This will try to load your **local** CSV (set `DATA_CSV_PATH` above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a28510",
      "metadata": {
        "id": "94a28510"
      },
      "outputs": [],
      "source": [
        "def load_dataset(local_path: str, fallback_url: str) -> pd.DataFrame:\n",
        "    if local_path and Path(local_path).exists():\n",
        "        print(f'Loading local dataset from: {local_path}')\n",
        "        return pd.read_csv(local_path)\n",
        "    print('Local dataset not found, falling back to URL.')\n",
        "    return pd.read_csv(fallback_url)\n",
        "\n",
        "df_raw = load_dataset(DATA_CSV_PATH, FALLBACK_URL)\n",
        "print('Shape:', df_raw.shape)\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ddb13c",
      "metadata": {
        "id": "71ddb13c"
      },
      "source": [
        "## 2. Quick EDA\n",
        "\n",
        "A fast look at numeric distributions and correlations. Modify as you like for deeper EDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285371d1",
      "metadata": {
        "id": "285371d1"
      },
      "outputs": [],
      "source": [
        "numeric_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = df_raw.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "display(pd.DataFrame({\n",
        "    'numeric': [numeric_cols],\n",
        "    'categorical': [cat_cols]\n",
        "}))\n",
        "\n",
        "# Basic histograms\n",
        "for col in [c for c in ['price','square_feet','bedrooms','bathrooms'] if c in df_raw.columns]:\n",
        "    df_raw[col].plot(kind='hist', bins=40, alpha=0.8, title=f'Distribution: {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "if len(numeric_cols) >= 2:\n",
        "    corr = df_raw[numeric_cols].corr()\n",
        "    plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.colorbar(label='correlation')\n",
        "    plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=90)\n",
        "    plt.yticks(range(len(numeric_cols)), numeric_cols)\n",
        "    plt.title('Correlation heatmap (numeric columns)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5d4cb0",
      "metadata": {
        "id": "3d5d4cb0"
      },
      "source": [
        "## 3. Cleaning & Preprocessing\n",
        "\n",
        "- **Missing values:** impute numerics with median, categoricals with most frequent.  \n",
        "- **Outliers:** winsorize numeric heavy tails via clipping to reasonable percentiles.  \n",
        "- **Encoding:** one-hot encode categoricals.  \n",
        "- **Scaling:** standardize numeric features.  \n",
        "- **Split:** stratified split if price is present; else random.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a02585a",
      "metadata": {
        "id": "8a02585a"
      },
      "outputs": [],
      "source": [
        "#  outlier clipping\n",
        "def clip_outliers(df: pd.DataFrame, cols, lower_q=0.01, upper_q=0.99):\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            lo, hi = df[c].quantile([lower_q, upper_q])\n",
        "            df[c] = df[c].clip(lo, hi)\n",
        "    return df\n",
        "\n",
        "target = 'price' if 'price' in df_raw.columns else None\n",
        "\n",
        "df = df_raw.copy()\n",
        "if target and df[target].isna().any():\n",
        "    # drop rows with no target\n",
        "    df = df[~df[target].isna()].copy()\n",
        "\n",
        "# Clip outliers in common numeric columns if they exist\n",
        "clip_candidates = [c for c in ['price','square_feet','bedrooms','bathrooms','latitude','longitude'] if c in df.columns]\n",
        "df = clip_outliers(df, clip_candidates, 0.01, 0.99)\n",
        "\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "if target and target in numeric_features:\n",
        "    numeric_features = [c for c in numeric_features if c != target]\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough',  # Keep other columns (like 'title', 'body', etc.) - although they won't be used in the model\n",
        ")\n",
        "\n",
        "\n",
        "# Train/Test split\n",
        "if target:\n",
        "    try:\n",
        "        bins = pd.qcut(df[target], q=10, duplicates='drop')\n",
        "        stratify_vec = bins\n",
        "    except Exception:\n",
        "        stratify_vec = None\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target].values\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify_vec\n",
        "    )\n",
        "else:\n",
        "    raise ValueError('Target column \"price\" is required for this project.')\n",
        "\n",
        "print(f'Train shape: {X_train.shape}, Test shape: {X_test.shape}')\n",
        "display(X_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba71f0e3",
      "metadata": {
        "id": "ba71f0e3"
      },
      "source": [
        "### Model Selection\n",
        "\n",
        "**choose which model to train** from the available options:  \n",
        "- **Linear Regression** â€“ a simple, interpretable baseline model.  \n",
        "- **HistGradientBoosting** â€“ a tree-based boosting model offering high performance.  \n",
        "- **Keras Neural Network** â€“ a feedforward deep learning model for complex relationships.  \n",
        "\n",
        "Select a model from the dropdown below to continue.  \n",
        "Your choice will be stored and used automatically in the next training step.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let the user pick which model to train\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "#  available model options\n",
        "model_options = [\n",
        "    \"Linear Regression\",\n",
        "    \"HistGradientBoosting\",\n",
        "    \"Keras Neural Network\"\n",
        "]\n",
        "\n",
        "print(\"Select a model to train:\")\n",
        "\n",
        "# model choice\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=model_options,\n",
        "    value=model_options[0],\n",
        "    description='Model:',\n",
        "    style={'description_width': '120px'},\n",
        "    layout=widgets.Layout(width='350px')\n",
        ")\n",
        "\n",
        "# Placeholder for displaying choice\n",
        "chosen_output = widgets.Output()\n",
        "\n",
        "def on_model_select(change):\n",
        "    with chosen_output:\n",
        "        clear_output()\n",
        "        print(f\"âœ… You selected: {change['new']}\")\n",
        "        # store the selection globally so next cell can access it\n",
        "        global selected_model\n",
        "        selected_model = change['new']\n",
        "\n",
        "model_dropdown.observe(on_model_select, names='value')\n",
        "display(model_dropdown, chosen_output)\n"
      ],
      "metadata": {
        "id": "1oz-uOZCoBt-"
      },
      "id": "1oz-uOZCoBt-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb50839",
      "metadata": {
        "id": "feb50839"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "\n",
        "target_col = \"price\" if \"price\" in df.columns else df.select_dtypes(include=[np.number]).columns[-1]\n",
        "y = df[target_col]\n",
        "X = df.drop(columns=[target_col])\n",
        "\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "numeric_tf = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "categorical_tf = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_tf, num_cols),\n",
        "    (\"cat\", categorical_tf, cat_cols)\n",
        "])\n",
        "\n",
        "# Fit preprocessing\n",
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "X_test_proc  = preprocessor.transform(X_test)\n",
        "\n",
        "# Train chosen model\n",
        "\n",
        "print(f\"ðŸš€ Training model: {selected_model}\")\n",
        "\n",
        "if selected_model == \"Linear Regression\":\n",
        "    model = LinearRegression()\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"model\", model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "elif selected_model == \"HistGradientBoosting\":\n",
        "    model = HistGradientBoostingRegressor(learning_rate=0.08, max_iter=300, random_state=42)\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"model\", model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "elif selected_model == \"Keras Neural Network\":\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(X_train_proc.shape[1],)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "    history = model.fit(\n",
        "        X_train_proc, y_train,\n",
        "        validation_split=0.15,\n",
        "        epochs=200, batch_size=64,\n",
        "        verbose=0,\n",
        "        callbacks=[keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)]\n",
        "    )\n",
        "    y_pred = model.predict(X_test_proc).reshape(-1)\n",
        "else:\n",
        "    raise ValueError(\"No valid model selected. Please run the first cell and choose a model.\")\n",
        "\n",
        "# Evaluation metrics\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae  = mean_absolute_error(y_test, y_pred)\n",
        "r2   = r2_score(y_test, y_pred)\n",
        "acc_tolerance = 0.10\n",
        "accuracy = np.mean(np.abs((y_pred - y_test) / np.maximum(np.abs(y_test), 1e-8)) <= acc_tolerance)\n",
        "\n",
        "print(\"\\\\nðŸ“Š Performance Metrics\")\n",
        "print(f\"RMSE: {rmse:,.2f}\")\n",
        "print(f\"MAE : {mae:,.2f}\")\n",
        "print(f\"RÂ²  : {r2:.3f}\")\n",
        "print(f\"Accuracy (Â±{int(acc_tolerance*100)}%): {accuracy*100:.2f}%\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Visualization\n",
        "\n",
        "# 1. Predicted vs Actual\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
        "plt.plot(lims, lims, \"r--\")\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(f\"Predicted vs Actual ({selected_model})\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Residuals Plot\n",
        "residuals = y_pred - y_test\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(y_pred, residuals, alpha=0.6)\n",
        "plt.axhline(0, color=\"r\", linestyle=\"--\")\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(f\"Residuals Plot ({selected_model})\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Error Distribution\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.hist(residuals, bins=30, color=\"gray\")\n",
        "plt.xlabel(\"Residuals (Prediction Error)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(f\"Error Distribution ({selected_model})\")\n",
        "plt.show()\n",
        "\n",
        "# 4.  NN training curve\n",
        "if selected_model == \"Keras Neural Network\":\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MSE Loss\")\n",
        "    plt.title(\"Neural Network Training Curve\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f43523",
      "metadata": {
        "id": "58f43523"
      },
      "source": [
        "### âš™ï¸ Data Preparation, Model Training & Evaluation\n",
        "\n",
        "This section handles the **core ML workflow** â€” preparing the data, training the chosen model, and visualizing its performance.\n",
        "\n",
        "**Steps performed:**\n",
        "1. **Data Loading & Splitting:**  \n",
        "   - Identifies the target column (`price`) and separates it from the feature set.  \n",
        "   - Splits data into training and testing sets (80/20 split).\n",
        "\n",
        "2. **Preprocessing Pipeline:**  \n",
        "   - Numerical features â†’ imputation (median) + scaling (`StandardScaler`)  \n",
        "   - Categorical features â†’ imputation (most frequent) + one-hot encoding  \n",
        "   - Combined using `ColumnTransformer`.\n",
        "\n",
        "3. **Model Training:**  \n",
        "   The user-selected model (`selected_model`) is trained:\n",
        "   - **Linear Regression** â€” interpretable baseline.  \n",
        "   - **HistGradientBoostingRegressor** â€” efficient tree-based ensemble.  \n",
        "   - **Keras Neural Network** â€” multi-layer perceptron with early stopping.\n",
        "\n",
        "4. **Evaluation Metrics:**  \n",
        "   After training, the model is evaluated on the test set using:\n",
        "   - **RMSE**, **MAE**, **RÂ²**, and **Accuracy (Â±10%)**  \n",
        "\n",
        "5. **Visualizations:**  \n",
        "   - Predicted vs Actual plot  \n",
        "   - Residuals plot  \n",
        "   - Error distribution  \n",
        "   - Neural network loss curves (for Keras model)\n",
        "\n",
        "These plots and metrics together provide a clear picture of each modelâ€™s performance and predictive reliability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6cba6c",
      "metadata": {
        "id": "4d6cba6c"
      },
      "outputs": [],
      "source": [
        "# 1ï¸ Accuracy under different tolerances (relative error)\n",
        "\n",
        "def regression_accuracy(y_true, y_pred, tol=0.10):\n",
        "    rel_error = np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8)\n",
        "    return np.mean(rel_error <= tol)\n",
        "\n",
        "tolerances = [0.05, 0.10, 0.20]\n",
        "for tol in tolerances:\n",
        "    acc = regression_accuracy(y_test, y_pred, tol)\n",
        "    print(f\"Accuracy within Â±{int(tol*100)}%: {acc*100:.2f}%\")\n",
        "\n",
        "# confusion matrix-like heatmap\n",
        "\n",
        "# Bin actual and predicted prices into categories\n",
        "n_bins = 6  # adjust as needed\n",
        "bins = np.linspace(y_test.min(), y_test.max(), n_bins)\n",
        "labels = [f\"${int(bins[i]):,}â€“${int(bins[i+1]):,}\" for i in range(len(bins)-1)]\n",
        "\n",
        "y_true_bins = pd.cut(y_test, bins=bins, labels=labels, include_lowest=True)\n",
        "y_pred_bins = pd.cut(y_pred, bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = pd.crosstab(y_true_bins, y_pred_bins, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
        "\n",
        "# Normalize to percentages\n",
        "cm_percent = cm.div(cm.sum(axis=1), axis=0) * 100\n",
        "\n",
        "#  Visualize the confusion matrix\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_percent, annot=True, fmt=\".1f\", cmap=\"Blues\", cbar_kws={\"label\": \"Percentage (%)\"})\n",
        "plt.title(f\"Confusion Matrix (Binned Prices) â€” {selected_model}\")\n",
        "plt.xlabel(\"Predicted Price Range $$$\")\n",
        "plt.ylabel(\"Actual Price Range $$$\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "print(\"\"\"\n",
        "ðŸ§¾ Interpretation:\n",
        "- Each cell shows the % of samples whose predicted price fell in that bin,\n",
        "  given the true price bin.\n",
        "- Values concentrated along the diagonal indicate more accurate predictions.\n",
        "- Off-diagonal cells show where the model over- or under-predicts price ranges.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb262da",
      "metadata": {
        "id": "2bb262da"
      },
      "source": [
        "###  Manual Apartment Price Prediction\n",
        "\n",
        "In this final step, the user can **manually input apartment details** (both numerical and categorical) directly into the console.  \n",
        "Once all features are entered, the trained model (`selected_model`) is used to predict the **estimated apartment price**.\n",
        "\n",
        "**Process overview:**\n",
        "1. Prompts the user to enter feature values one by one.  \n",
        "2. Handles both numeric and categorical inputs safely.  \n",
        "3. Converts inputs into a `DataFrame` and applies the same preprocessing pipeline.  \n",
        "4. Uses the selected trained model to generate a **price prediction**.  \n",
        "5. Displays the entered details and the **predicted apartment price ðŸ’°**.\n",
        "\n",
        "This interactive step allows anyone to test the trained model on **custom apartment configurations** in real time.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Manual Apartment Price Prediction\n",
        "\n",
        "print(f\"ðŸ’¡ Using trained model: {selected_model}\")\n",
        "\n",
        "# user inputs from the console\n",
        "user_data = {}\n",
        "print(\"\\nPlease enter the apartment details below:\")\n",
        "\n",
        "for col in X.columns:\n",
        "    if col in cat_cols:\n",
        "        # For categorical features\n",
        "        val = input(f\"Enter value for '{col}' (text): \").strip()\n",
        "        if val == \"\":\n",
        "            val = np.nan\n",
        "        user_data[col] = val\n",
        "    else:\n",
        "        # For numeric features\n",
        "        while True:\n",
        "            val = input(f\"Enter value for '{col}' (numeric): \").strip()\n",
        "            if val == \"\":\n",
        "                val = np.nan\n",
        "                break\n",
        "            try:\n",
        "                val = float(val)\n",
        "                break\n",
        "            except ValueError:\n",
        "                print(\"âš ï¸ Please enter a valid number.\")\n",
        "        user_data[col] = val\n",
        "\n",
        "# Convert to DataFrame\n",
        "user_df = pd.DataFrame([user_data])\n",
        "\n",
        "print(\"\\nðŸ™ï¸ Apartment features entered:\")\n",
        "display(user_df)\n",
        "\n",
        "#  prediction using the trained model\n",
        "\n",
        "try:\n",
        "    if selected_model == \"Keras Neural Network\":\n",
        "        user_proc = preprocessor.transform(user_df)\n",
        "        predicted_price = model.predict(user_proc, verbose=0)[0][0]\n",
        "    elif selected_model in [\"Linear Regression\", \"HistGradientBoosting\"]:\n",
        "        user_proc = preprocessor.transform(user_df)\n",
        "        predicted_price = model.predict(user_proc)[0]\n",
        "    else:\n",
        "        raise ValueError(\"No valid trained model found. Please rerun training.\")\n",
        "except Exception as e:\n",
        "    print(\"âŒ Error during prediction:\", str(e))\n",
        "else:\n",
        "    print(f\"\\nðŸ’° **Predicted Apartment Price:** ${predicted_price:,.2f}\")\n"
      ],
      "metadata": {
        "id": "qneBBbeXrq2z"
      },
      "id": "qneBBbeXrq2z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1e8f9287",
      "metadata": {
        "id": "1e8f9287"
      },
      "source": [
        "### ðŸ Conclusion & Next Steps\n",
        "\n",
        "**Summary**  \n",
        "- The notebook successfully demonstrates an **end-to-end apartment price prediction pipeline**, covering preprocessing, model selection, training, evaluation, and user interaction.  \n",
        "- Among the models tested, the **best-performing one** (see results above) offers a strong balance between predictive accuracy and interpretability.  \n",
        "- Interactive selection and manual input make this project both **educational and practical**.\n",
        "\n",
        "**Potential Improvements**  \n",
        "- Implement **automated hyperparameter tuning** (e.g., `GridSearchCV`, Optuna) to optimize model performance.  \n",
        "- Explore **feature engineering** â€” polynomial or interaction terms, and domain-specific transformations.  \n",
        "- Apply **target transformations** (e.g., log-scaling) if price distribution is heavily skewed.  \n",
        "- Experiment with **advanced gradient boosting** methods such as XGBoost, LightGBM, or CatBoost.  \n",
        "- Introduce **cross-validation strategies** aware of time or location if the dataset has temporal/spatial context.  \n",
        "- Add **prediction intervals** (via quantile regression or conformal prediction) to quantify model uncertainty.  \n",
        "\n",
        "This project provides a solid foundation for more advanced real estate price prediction systems and is an excellent showcase of applied machine learning workflow design.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}